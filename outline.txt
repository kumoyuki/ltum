-*- outline -*-

* Lisp as Lambda Calculus: A historical perspective
** McCarthy 1960
   _Recursive functions of symbolic expressions and their computation by machine_
** Lisp is not lambda calculus
   https://danielsz.github.io/2019-08-05t21_14.html
** Scheme is not Lisp

* Why does Scheme have macros
** LISP 1.5, R4RS, and Racket
*** talk about SUBR, FSUBR, EXPR, FEXPR, and MACRO
*** legacy as a tool for computing about logical propositions
*** programs are full of logical propositions
*** insufficiently smart compilers

* The happy accident of S-Expressions
** LISP privilege: fooling ourselves about surface syntax

** code is data (homoiconic) is a type error
This idea has room for a lot of exploration, probably need to check
with papers from PLT/Racket on their compilation phases.

*** semantics of EVAL :: IO AST -> IO a -> IO b?

Denotational semantics separates the notion of surface syntax from the
actual semantic domains involved in a computation. This simplifies
reasoning about certain kinds of primitives. In a DS we can reify
syntax trees into the set of semantic domains in order to write
program transformers. But once an AST has been converted to a function,
the function has no access to it's own AST. The implication is that
macro extensions require a recursive EVAL where I'm not sure I know
how to write down a type. Macro defining forms are a kind of
fixed-point combinator that allows the extension of EVAL with
functions M :: AST -> AST.

But operational/reduction semantics tries to treat everything as
syntax transformations. This means that any macro is effectively an
extension to the rules of the reduction machine. In principle, you'd
have to prove that your extension is both well-behaved on its own and
in the context of the rest of the reduction rules. SYNTAX-RULES and
SYNTAX-CASE attempt to ensure well-behaved extensions of EVAL by
limiting macros to finite tree traversals, but I'm pretty sure that
Al* Petrofsky or Oleg Kiselyov showed that SYNTAX-RULES is actually
turing complete, so OOPS.

Reynolds, in _Theory of Programming Languages_ section 10.5 points out
that untyped lambda calculi produce paradoxes. And so we come back to
some kind of typed reduction semantics, which contains a type for
reduction rules. How would that even work?

I feel like all of this points at quasiquote as being the fundamental
macro primitive. That being said, quasiquote has EVAL embedded in it, so
perhaps it is actually not different at all from define-macro and
syntax-rules when viewed through the lens of DS

**** Denotational vs Reduction semantics

The more I think about it, the more it seems like there is probably a
whole paper (maybe two) in figuring out how to semantically model
lambda-calculi with macros. As a brief sketch consider that DS is a
Harvard architecture model and a lot like a turing machine that keeps
the program text separated from the actual data which is being
manipulated; while a RS has much more of a Von Neuman architecture
that blurs the lines between text and semantic domains by defining the
semantics solely as transformations to the program text.

On some level this makes RS inherently more Lisp-y because
homoiconicity is part of the construction. But it also seems
problematic for dealing with I/O, and I think it might be
fundamentally unable to deal with the reflection problems that macros
bring, at least as it is commonly formulated.

The EVAL with macros type problem for DS that I referred to previously
seems like it ought to be solvable, if only because we actually have a
way to talk about the AST->semantic transformation in
DS. Nevertheless, I could easily see the possible necessity of a
Y-combinator like entity needing to be invented to close the loop, and
that would need to have its soundness proven.

It seems likely that these two semantic specification styles would
speak to each other, and that a solution to one would be translatable
to the other. While writing about the DS type problem I find myself
conjuring up ways to tag RS meta-constructs so that a copy of the RS
can be run on them in a quasi-separate step.

**** Reduction Semantics for QuasiQuote

At some point we are going to need to do some whiteboarding and/or
code sketches. But as mentioned earlier this might be going beyond the
scope of an anti-macro manifesto. Or am I just overthinking all of
this?

But then there is this https://weinholt.se/articles/quasiquote-literal-magic/
which points to the fundamental weirdness of QQ in R[67]RS. Personally,
I think this indicates just how badly R6+RS lost the plot, but it also
shows that a solid semantic basis for QQ might be actually useful to
the Scheme community (assuming no one has done it yet).

The first task is trying to define the language to use for the
RS. Obviously, this will not be RnRS for any 'n'. but something like

exp ::= constant
      | VAR
      | LAMBDA VAR exp
      | exp exp
      | qqx

constant ::= SCALAR | FUNCTION

qqx ::= QQ qq-body
qq-body ::= constant
          | qq-body constant
          | UQ exp
          | UQ-SPLICING exp

ought to work. Tokens are specified in ALL-CAPS. SCALAR designates the
infinite set of all symbols and numbers, while FUNCTION refers to the
finite set of known basis functions.

** macros are as bad for LISP as they are for any other language

* Macros versus special forms
** typical use case
** david's functor macro
** SRFI 247: Syntactic Monads
*** also Ed Kmett's https://github.com/ekmettscheme-monads
*** basic monad expression in Scheme
** destructuring-bind
*** auric scheme
*** make-cons special form

* What exactly are we trying to do with macros
** brevity of expression
** meta-programming
*** HOFs are better
*** Quasi-quote is enough
** eager vs lazy evaluation

* From Quasiquote to Macros

** How to extend a reduction machine from the inside

You may not remember, but I conjectured back in 2013-ish that
Church-Rosser reduction machines are composable if they are defined
over the same/compatible source languages. It still seems obvious to
me, but you suggested that a proof might be required for this
idea. Contrasting QQ and  Macros makes define-macro look like an
implemetation of this concept, so it is probably worth revisiting a
proof. I have thought about it at various times over the years, but my
latest proof outline is buried in offline cache just now.

** Reduction Semantics for define-macro

Oddly, I think it is more straightforward to define the semantics for
define-macro than for syntax-rules. I think that is mostly because
syntax-rules reifies the underlying reduction machine, making it a
more complex structure. If we want to make the point about getting rid
of define-macro, then we pretty much need a weird little language like
the one in the Reduction Semantics for Quasiquote section.

* And then there was Kernel

https://web.cs.wpi.edu/~jshutt/kernel.html

John Shutt's dissertation on the vau-calculus (we seriously need to
obliterate that name) attempts a rigorous approach to the
whole problem of homoiconicity. I haven't read enough of his
dissertation to poke holes in it yet, but in section 1.2.3.2
"Quotation" he writes:

  The quotation device, in its simplest form, is an explicit operator
  designating that its operand is to be used without evaluation. An
  explicit operator usually does something (i.e., designates an
  action), and so one may naturally suppose that the quotation
  operator suppresses evaluation of its operand. One is thus guided
  subtly into a conceptual framework in which all subexpressions are
  evaluated before use unless explicitly otherwise designated by
  context.

which not only speaks directly to our thesis about replacing macros by
quasiquote, but also to the underlying necessity for a canonical order
of evaluation (outermost/leftmost) to ensure a reduction to canonical
form. He's got about 200 pages of semantics which will need to be
digested in order to to see what he actually does about normal order
reduction.

The dissertation is available as

Shutt10
Fexprs as the basis of Lisp function application or $vau : the ultimate abstraction
https://digital.wpi.edu/concern/etds/ht24wj541
https://klisp.org/wp-content/uploads/2023/07/jshutt.pdf

since the link on his Kernel Programming Language page no longer works.
Apparently WPI reorganized their doctoral pubs, 

Shutt09
Revised-1 Report on the Kernel Programming Language
https://ftp.cs.wpi.edu/pub/techreports/pdf/05-07.pdf

<digression>

His 1993 MSc thesis on Recursive Adaptable Grammars (which has an
eerie resonance with the PhD work on Scheme with formalized
meta-programming) is easily available in a disused broom closet of an
FTP server where the Kernel report resides.

Shutt93
Recursive Adaptable Grammars
https://ftp.cs.wpi.edu/pub/projects_and_papers/theory/Shutt-thesis93.pdf

</digression>

Apparently the vau-calculus thesis was moderately inspiring during the
early 20-teens and there is a page titled The Kernel Underground that
lists 18-ish implementations of the language. a quick random sampling
of the associated repos seems like most of the interest died down by
2015. I may need to grab one of the more completely functioning ones
and write some code for it ;)

** (equal? Quasiquote macro) => #t

I am just about convinced that Shutt's thesis effectively proves a
theorem which makes this a trivial corollary. So what is there that is
interesting here?

*** worked examples that just use quasiquote
*** maybe you don't need the full power of Kernel to get what we want

* Further work

I've run across a number of references in the work around Kernel that
contend that it can be implemented efficiently with aggressive partial
evaluation. As this is a recurring theme in the BCL world (are we
still in that world?), there might be something in trying to push the
Kernel boat further into becoming a useful platform. I guess that we'd
need some application where all the extra stuff actually helped.

At the risk of beating a dead horse: AD seems like it might be a
natural fit for Kernel. And it could be a big win if the whole thing
could be strongly typed.
