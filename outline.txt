-*- outline -*-

* Lisp as Lambda Calculus: A historical perspective
** McCarthy 1960
   _Recursive functions of symbolic expressions and their computation by machine_
** Lisp is not lambda calculus
   https://danielsz.github.io/2019-08-05t21_14.html
** Scheme is not Lisp

* Why does Scheme have macros
** LISP 1.5, R4RS, and Racket
*** talk about SUBR, FSUBR, EXPR, FEXPR, and MACRO
*** legacy as a tool for computing about logical propositions
*** programs are full of logical propositions
*** insufficiently smart compilers

* The happy accident of S-Expressions
** LISP privilege: fooling ourselves about surface syntax

** code is data (homoiconic) is a type error
This idea has room for a lot of exploration, probably need to check
with papers from PLT/Racket on their compilation phases.

*** semantics of EVAL :: IO AST -> IO a -> IO b?

Denotational semantics separates the notion of surface syntax from the
actual semantic domains involved in a computation. This simplifies
reasoning about certain kinds of primitives. In a DS we can reify
syntax trees into the set of semantic domains in order to write
program transformers. But once an AST has been converted to a function,
the function has no access to it's own AST. The implication is that
macro extensions require a recursive EVAL where I'm not sure I know
how to write down a type. Macro defining forms are a kind of
fixed-point combinator that allows the extension of EVAL with
functions M :: AST -> AST.

But operational/reduction semantics tries to treat everything as
syntax transformations. This means that any macro is effectively an
extension to the rules of the reduction machine. In principle, you'd
have to prove that your extension is both well-behaved on its own and
in the context of the rest of the reduction rules. SYNTAX-RULES and
SYNTAX-CASE attempt to ensure well-behaved extensions of EVAL by
limiting macros to finite tree traversals, but I'm pretty sure that
Al* Petrofsky or Oleg Kiselyov showed that SYNTAX-RULES is actually
turing complete, so OOPS.

Reynolds, in _Theory of Programming Languages_ section 10.5 points out
that untyped lambda calculi produce paradoxes. And so we come back to
some kind of typed reduction semantics, which contains a type for
reduction rules. How would that even work?

I feel like all of this points at quasiquote as being the fundamental
macro primitive.

**** Denotational vs Reduction semantics

The more I think about it, the more it seems like there is probably a
whole paper (maybe two) in figuring out how to semantically model
lambda-calculi with macros. As a brief sketch consider that DS is a
Harvard architecture model and a lot like a turing machine that keeps
the program text separated from the actual data which is being
manipulated; while a RS has much more of a Von Neuman architecture
that blurs the lines between text and semantic domains by defining the
semantics solely as transformations to the program text.

On some level this makes RS inherently more Lisp-y because
homoiconicity is part of the construction. But it also seems
problematic for dealing with I/O, and I think it might be
fundamentally unable to deal with the reflection problems that macros
bring, at least as it is commonly formulated.

The EVAL with macros type problem for DS that I referred to previously
seems like it ought to be solvable, if only because we actually have a
way to talk about the AST->semantic transformation in
DS. Nevertheless, I could easily see the possible necessity of a
Y-combinator like entity needing to be invented to close the loop, and
that would need to have its soundness proven.

It seems likely that these two semantic specification styles would
speak to each other, and that a solution to one would be translatable
to the other. While writing about the DS type problem I find myself
conjuring up ways to tag RS meta-constructs so that a copy of the RS
can be run on them in a quasi-separate step.

At some point we are going to need to do some whiteboarding and/or
code sketches. But as mentioned earlier this might be going beyond the
scope of an anti-macro manifesto. Or am I just overthinking all of
this?


** macros are as bad for LISP as they are for any other language

* Macros versus special forms
** typical use case
** david's functor macro
** SRFI 247: Syntactic Monads
*** also Ed Kmett's https://github.com/ekmettscheme-monads
*** basic monad expression in Scheme
** destructuring-bind
*** auric scheme
*** make-cons special form

* What exactly are we trying to do with macros
** brevity of expression
** meta-programming
*** HOFs are better
*** Quasi-quote is enough
** eager vs lazy evaluation

* (equal? Quasiquote macro) => #t
